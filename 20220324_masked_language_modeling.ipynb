{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20220324_masked_language_modeling",
      "provenance": [],
      "authorship_tag": "ABX9TyOYOGkuBHOr0Degy0Kuhdkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsato-code/bert/blob/main/20220324_masked_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# このノートブックの概要\n",
        "\n",
        "- ストックマーク社の BERT 本5章を動作確認。\n",
        "- 内容は文章の穴埋め。\n",
        "\n",
        "TODO\n",
        "- そもそも文章の穴埋めをしたいのはどのようなときか。"
      ],
      "metadata": {
        "id": "9LN9X6aLg-F1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4on7u6OAgd8B",
        "outputId": "1b83dc42-2484-4bb8-c56e-f99873a18ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 486 kB 35.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 57.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 50.8 MB/s \n",
            "\u001b[?25h  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "### fugashi は 形態素解析ツール Mecab を Python から使えるようにしたもの\n",
        "### ipadic は Mecab で形態素解析を利用するときに使う辞書\n",
        "!pip install -q transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### グローバル変数\n",
        "JAPANESE_WIKI_MODEL = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
      ],
      "metadata": {
        "id": "kOZHIfIHhIZ1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ライブラリのインポート\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM"
      ],
      "metadata": {
        "id": "pSP4l_tPhK6X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 準備\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(JAPANESE_WIKI_MODEL)\n",
        "bert_mlm = BertForMaskedLM.from_pretrained(JAPANESE_WIKI_MODEL)\n",
        "bert_mlm = bert_mlm.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERkNP4ErhU77",
        "outputId": "c2bef846-ee82-4df1-a191-ab77f6f9f69c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '[MASK]の夜明けぜよ'\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxLJ1hlehuUB",
        "outputId": "8951e852-37cd-4af8-f0a6-070d67c1893a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[MASK]', 'の', '夜明け', 'ぜ', 'よ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 符号化\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "input_ids = input_ids.cuda()\n",
        "\n",
        "### 分類スコア\n",
        "with torch.no_grad():\n",
        "    output = bert_mlm(input_ids=input_ids)\n",
        "scores = output.logits\n",
        "\n",
        "print(scores.shape)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzz__uQ2iCCL",
        "outputId": "6ce0e7d0-9b5a-48b9-e172-0c978e9a8425"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 12, 32000])\n",
            "tensor([[[-4.8918,  4.6949, -3.5190,  ..., -4.3858, -3.9782, -2.9297],\n",
            "         [-4.2450,  5.8047, -3.1998,  ..., -3.4704, -6.0453, -3.4959],\n",
            "         [-3.6191,  7.0827, -3.8722,  ..., -6.0576, -6.2808, -6.6423],\n",
            "         ...,\n",
            "         [-4.3523,  5.3218, -3.8007,  ..., -4.0504, -3.5354, -2.5074],\n",
            "         [-4.1096,  5.1667, -3.7727,  ..., -3.7396, -3.7238, -2.1865],\n",
            "         [-6.4411,  6.2043, -6.2109,  ..., -3.5276, -5.3214, -7.1897]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### [MASK] の位置\n",
        "mask_position = input_ids[0].tolist().index(4)\n",
        "print(mask_position)\n",
        "\n",
        "### 最大スコアのトークン id をもとにトークンに変換\n",
        "id_best = scores[0, mask_position].argmax().item()\n",
        "token_best = tokenizer.convert_ids_to_tokens(id_best)\n",
        "token_best = token_best.replace('##', '')\n",
        "print(token_best)\n",
        "\n",
        "### 元文を置換\n",
        "print(text.replace('[MASK]', token_best))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMfHPTLkibnS",
        "outputId": "462f9305-5ad1-4170-f37a-d71a51d3c64e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "明日\n",
            "明日の夜明けぜよ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mask_topk(text, tokenizer, bert_mlm, num_topk):\n",
        "    ### 符号化\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "    input_ids = input_ids.cuda()\n",
        "\n",
        "    ### 分類スコア\n",
        "    with torch.no_grad():\n",
        "        output = bert_mlm(input_ids=input_ids)\n",
        "    scores = output.logits\n",
        "    \n",
        "    ### スコア上位のトークン id をもとにトークンに変換\n",
        "    mask_position = input_ids[0].tolist().index(4)\n",
        "    topk = scores[0, mask_position].topk(num_topk)\n",
        "    ids_topk = topk.indices\n",
        "    tokens_topk = tokenizer.convert_ids_to_tokens(ids_topk)\n",
        "    scores_topk = topk.values.cpu().numpy()\n",
        "\n",
        "    ### スコア上位のトークンで元文を置換\n",
        "    text_topk = []\n",
        "    for token in tokens_topk:\n",
        "        token = token.replace('##', '')\n",
        "        ### print(token)\n",
        "        text_topk.append(text.replace('[MASK]', token, 1))\n",
        "\n",
        "    return text_topk, scores_topk\n",
        "\n",
        "text = '知床の海に[MASK]'\n",
        "text_topk, _ = predict_mask_topk(text, tokenizer, bert_mlm, 10)\n",
        "print(*text_topk, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-rNN5d9jFH7",
        "outputId": "92d6a634-5315-42da-ae8d-9ea5577853a0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "知床の海に浮かぶ\n",
            "知床の海に沈む\n",
            "知床の海に。\n",
            "知床の海にある\n",
            "知床の海に面する\n",
            "知床の海に...\n",
            "知床の海に面し\n",
            "知床の海に立つ\n",
            "知床の海に[UNK]\n",
            "知床の海に架かる\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 複数の [MASK] を前から貪欲法で埋める\n",
        "def greedy_prediction(text, tokenizer, bert_mlm):\n",
        "    for _ in range(text.count('[MASK]')):\n",
        "        text = predict_mask_topk(text, tokenizer, bert_mlm, 1)[0][0]\n",
        "    return text\n",
        "\n",
        "\n",
        "text = '[MASK]の[MASK]に[MASK]'\n",
        "greedy_prediction(text, tokenizer, bert_mlm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Tdx00MNCn3dV",
        "outputId": "a673cd9c-47a0-488d-8d37-55a854a9cd31"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'社会のために。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### [MASK] が多すぎると苦手\n",
        "text = '個人情報保護法が[MASK][MASK][MASK][MASK][MASK]'\n",
        "greedy_prediction(text, tokenizer, bert_mlm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "znJ_IKkdpGbr",
        "outputId": "fed0899e-e8c4-44bf-e3a9-396120c5be72"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'個人情報保護法が社会的に成立。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 複数の [MASK] をビームサーチで埋める\n",
        "def beam_search(text, tokenizer, bert_mlm, num_topk):\n",
        "    num_mask = text.count('[MASK]')\n",
        "    text_topk = [text]\n",
        "    scores_topk = np.array([0])\n",
        "    for _ in range(num_mask):\n",
        "        text_candidates = []\n",
        "        score_candidates = []\n",
        "        for text_mask, score in zip(text_topk, scores_topk):\n",
        "            text_topk_inner, scores_topk_inner = predict_mask_topk(\n",
        "                text_mask, tokenizer, bert_mlm, num_topk)\n",
        "            text_candidates.extend( text_topk_inner )\n",
        "            score_candidates.append( score + scores_topk_inner )\n",
        "        \n",
        "        score_candidates = np.hstack(score_candidates)\n",
        "        idx_list = score_candidates.argsort()[::-1][:num_topk]\n",
        "        text_topk = [text_candidates[idx] for idx in idx_list]\n",
        "        scores_topk = score_candidates[idx_list]\n",
        "    \n",
        "    return text_topk\n",
        "\n",
        "\n",
        "text_topk = beam_search(text, tokenizer, bert_mlm, 20)\n",
        "print(*text_topk, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLDZ9AUJplIr",
        "outputId": "9f368bb0-6589-4184-82de-a662c1323d4d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "個人情報保護法が社会化される。\n",
            "個人情報保護法が社会に広がった。\n",
            "個人情報保護法が社会問題となる。\n",
            "個人情報保護法が社会問題化する。\n",
            "個人情報保護法が社会問題になる。\n",
            "個人情報保護法が社会に広まる。\n",
            "個人情報保護法が社会化された\n",
            "個人情報保護法が社会問題となった\n",
            "個人情報保護法が社会に広まった。\n",
            "個人情報保護法が社会化した。\n",
            "個人情報保護法が2位である。\n",
            "個人情報保護法が社会問題になった\n",
            "個人情報保護法が社会問題に発展。\n",
            "個人情報保護法が社会問題化した\n",
            "個人情報保護法が社会化され、\n",
            "個人情報保護法が社会化され。\n",
            "個人情報保護法が社会問題となり、\n",
            "個人情報保護法が社会化されると\n",
            "個人情報保護法が社会問題となり。\n",
            "個人情報保護法が社会に広まり。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zk2iqp0arZEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}